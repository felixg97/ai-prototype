{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# custom\n",
    "from utils.utils import load_local_dataset_train_test, load_local_dataset, load_local_dataset_tf, preprocess_data_per_tfmodel\n",
    "from utils.constants import SOURCE_DATASETS, TARGET_DATASETS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 181 files belonging to 2 classes.\n",
      "Using 145 files for training.\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "Found 181 files belonging to 2 classes.\n",
      "Using 36 files for validation.\n",
      "batches:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 21:57:35.751854: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-02-04 21:57:35.752008: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "TARGET_SIZE = (300, 300)\n",
    "# BATCH_SIZE = 128\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "\n",
    "path = \"/Users/felixgerschner/git/ai-prototype/data/target/mechanicalseals_fulllight\"\n",
    "# path = \"/Users/felixgerschner/git/ai-prototype/data/source/miniimagenet\"\n",
    "# path = \"/Users/felixgerschner/git/ai-prototype/data/source/dagm\"\n",
    "\n",
    "\n",
    "train = load_local_dataset_tf(path, target_size=TARGET_SIZE, subset=\"training\", batch_size=32)\n",
    "test = load_local_dataset_tf(path, target_size=TARGET_SIZE, subset=\"test\", batch_size=32)\n",
    "\n",
    "# X, y = load_local_dataset(path, \".png\", target_size=TARGET_SIZE)\n",
    "\n",
    "print(\"batches: \", train.cardinality().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 300, 300, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 300, 300, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 300, 300, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 150, 150, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 150, 150, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 150, 150, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 75, 75, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 75, 75, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 75, 75, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 75, 75, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 37, 37, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 37, 37, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 37, 37, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 37, 37, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 18, 18, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 41472)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                1327136   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " predictions (Dense)         (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,042,946\n",
      "Trainable params: 1,328,258\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# X = keras.applications.vgg16.preprocess_input(dataset[0])\n",
    "\n",
    "train_preprocessed = train.take(5)\n",
    "test_preprocessed = test.take(5)\n",
    "\n",
    "train_preprocessed = preprocess_data_per_tfmodel(train)\n",
    "test_preprocessed = preprocess_data_per_tfmodel(test)\n",
    "\n",
    "classes = train.class_names\n",
    "\n",
    "\n",
    "# this could also be the output a different Keras model or layer\n",
    "# input_tensor = keras.layers.Input(shape=(224, 224, 3), name=\"put it in\")\n",
    "\n",
    "base_model = keras.applications.VGG16(\n",
    "            weights=\"imagenet\",\n",
    "            input_shape=(*TARGET_SIZE, 3),\n",
    "            include_top=False,\n",
    "        )\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = base_model.layers[-1].output\n",
    "x = keras.layers.Flatten()(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "x = keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "\n",
    "out = keras.layers.Dense(len(classes), activation=\"softmax\", name=\"predictions\")(x)\n",
    "\n",
    "model = keras.models.Model(\n",
    "            inputs=base_model.input, \n",
    "            outputs=out)\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=1e-4)\n",
    "\n",
    "model.compile(\n",
    "    optimizer = optimizer,\n",
    "    loss = \"categorical_crossentropy\",\n",
    "    metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# images, labels = next(iter(train_preprocessed))\n",
    "\n",
    "# x_image, y_image = X[0], y[0]\n",
    "\n",
    "\n",
    "# print(y_image)\n",
    "# plt.imshow(x_image.astype(\"uint8\"))\n",
    "# plt.axis(\"off\")\n",
    "\n",
    "# x_image = np.expand_dims(x_image, axis=0)\n",
    "# images = np.expand_dims(images, axis=0)\n",
    "\n",
    "# print(images.shape, \" \", labels.shape)\n",
    "\n",
    "# plt.imshow(images.numpy().astype(\"uint8\"))\n",
    "\n",
    "# _image = image.img_to_array(images)\n",
    "\n",
    "# print(_image.shape)\n",
    "\n",
    "# y_pred = model.predict(images)\n",
    "\n",
    "# print(labels)\n",
    "# print(np.argmax(y_pred))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# model.predict(images)\n",
    "\n",
    "# y_pred = model.predict(images)\n",
    "# print(y_pred)\n",
    "\n",
    "# for image, label in preprocessed.take(1):\n",
    "#     print(\"Label: \", label.numpy())\n",
    "#     print(\"Image shape: \", image.numpy().shape)\n",
    "#     plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "#     plt.axis(\"off\")\n",
    "# import time\n",
    "\n",
    "# start_time = time.perf_counter()\n",
    "# for epoch_num in range(2):\n",
    "#     for sample in dataset:\n",
    "#         # Performing a training step\n",
    "#         time.sleep(0.01)\n",
    "# print(\"Execution time:\", time.perf_counter() - start_time)\n",
    "    \n",
    "#     print(type(images[0]))\n",
    "#     image = keras.applications.vgg16.preprocess_input(images[1])\n",
    "#     print(type(image))\n",
    "    \n",
    "\n",
    "\n",
    "# keras.applications.vgg16.preprocess_input(X_train)\n",
    "\n",
    "\n",
    "# path = \"/Users/felixgerschner/git/ai-prototype/data/source/caltech101\"\n",
    "# load_local_dataset_tf(path, target_size=TARGET_SIZE)\n",
    "\n",
    "\n",
    "# class_names = data.class_names\n",
    "# print(class_names)\n",
    "\n",
    "# for images, labels in data.take(1):\n",
    "#     print(images.shape, \" \", labels.shape)\n",
    " \n",
    "#     print(labels[0])\n",
    "#     plt.imshow(images[10].numpy().astype(\"uint8\"))\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "\n",
    "# X, y = load_local_dataset(path, img_format, target_size=TARGET_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(None, 300, 300, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(\n",
    "        train,\n",
    "        epochs=2,\n",
    "        validation_data=test,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "print(hist.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, \" \", layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-04 21:58:41.646737: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-02-04 21:58:41.700898: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 654ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.59885099e-03, 9.97401118e-01],\n",
       "       [5.48819043e-02, 9.45118070e-01],\n",
       "       [8.38409215e-02, 9.16159093e-01],\n",
       "       [2.64571444e-03, 9.97354269e-01],\n",
       "       [1.42270923e-02, 9.85772908e-01],\n",
       "       [1.66414812e-01, 8.33585203e-01],\n",
       "       [3.50759854e-03, 9.96492445e-01],\n",
       "       [5.42987662e-04, 9.99457061e-01],\n",
       "       [2.35533610e-01, 7.64466405e-01],\n",
       "       [1.19452393e-02, 9.88054812e-01],\n",
       "       [7.70815508e-03, 9.92291868e-01],\n",
       "       [2.16640189e-01, 7.83359826e-01],\n",
       "       [1.82219793e-03, 9.98177767e-01],\n",
       "       [3.81089840e-03, 9.96189177e-01],\n",
       "       [2.50497391e-03, 9.97495055e-01],\n",
       "       [1.15254559e-02, 9.88474548e-01],\n",
       "       [4.89468360e-03, 9.95105267e-01],\n",
       "       [1.44662619e-01, 8.55337322e-01],\n",
       "       [1.15088405e-04, 9.99884844e-01],\n",
       "       [5.15779806e-03, 9.94842231e-01],\n",
       "       [1.41536153e-03, 9.98584628e-01],\n",
       "       [3.31569090e-02, 9.66843069e-01],\n",
       "       [1.16139022e-03, 9.98838603e-01],\n",
       "       [2.47534700e-02, 9.75246489e-01],\n",
       "       [4.77572856e-03, 9.95224297e-01],\n",
       "       [7.29054436e-02, 9.27094519e-01],\n",
       "       [7.08796531e-02, 9.29120362e-01],\n",
       "       [2.17764638e-03, 9.97822404e-01],\n",
       "       [1.83297887e-01, 8.16702127e-01],\n",
       "       [6.64259493e-02, 9.33574021e-01],\n",
       "       [7.52770156e-02, 9.24723029e-01],\n",
       "       [9.69568267e-02, 9.03043151e-01]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[-4].name\n",
    "\n",
    "test_input = test_preprocessed.take(1)\n",
    "\n",
    "model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "image, label = next(iter(test_preprocessed))\n",
    "\n",
    "y_pred = model.predict(np.expand_dims(image[0], axis=0))\n",
    "\n",
    "\n",
    "pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "acc = accuracy_score([1], pred)\n",
    "print(acc)\n",
    "\n",
    "print(np.argmax(label, axis=1))\n",
    "print(pred)\n",
    "print(image[0].shape)\n",
    "print(len(image[0].shape))\n",
    "print(y_pred)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image[0].numpy().astype(\"uint8\"))\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate([y for x, y in test_preprocessed], axis=0)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = tf.keras.utils.image_dataset_from_directory(\n",
    "#   path,\n",
    "#   validation_split=0.2,\n",
    "#   subset=\"training\",\n",
    "#   seed=123,\n",
    "#   image_size=TARGET_SIZE,\n",
    "#   batch_size=BATCH_SIZE,\n",
    "#   interpolation=\"area\"\n",
    "# )\n",
    "\n",
    "\n",
    "# for images, labels in data.take(1):\n",
    "#     print(images.shape, \" \", labels.shape)\n",
    " \n",
    "#     print(labels[1])\n",
    "#     plt.imshow(images[0].numpy().astype(\"uint8\"))\n",
    "#     plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(X[4].astype(\"uint8\"))\n",
    "# plt.axis(\"off\")\n",
    "# X[4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.tf_models.VGG16_model import VGG16_model\n",
    "\n",
    "\n",
    "# model = VGG16_model(\"imagenet\", input_shape=(350, 350, 3), \n",
    "#                 num_classes=2, build_pre_model=True)\n",
    "\n",
    "# model.fit_and_save_pre_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.tf_models.ResNet101_model import ResNet101_model\n",
    "\n",
    "\n",
    "# model = ResNet101_model(\"imagenet\", input_shape=(350, 350, 3), \n",
    "#                 num_classes=2, build_pre_model=True)\n",
    "\n",
    "# model.fit_and_save_pre_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.tf_models.DenseNet121_model import DenseNet121_model\n",
    "\n",
    "\n",
    "# model = DenseNet121_model(\"imagenet\", input_shape=(350, 350, 3), \n",
    "#                 num_classes=2, build_pre_model=True)\n",
    "\n",
    "# model.fit_and_save_pre_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from models.tf_models.MobileNet_model import MobileNet_model\n",
    "\n",
    "\n",
    "# model = MobileNet_model(\"mechanicalseal_fulllight\", input_shape=(350, 350, 3), \n",
    "#                 num_classes=2, build_pre_model=True)\n",
    "\n",
    "# model.fit_and_save_pre_model(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(X[2], cmap=\"binary\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "# model = VGG16(weights=\"imagenet\", include_top=True)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf \n",
    "\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # custom\n",
    "# from utils.utils import load_local_dataset, load_local_dataset_train_test, preload_from_directory_old\n",
    "\n",
    "# path = \"/Users/felixgerschner/git/ai-prototype/data/source/miniimagenet\" # CRASHES KERNEL\n",
    "# path = \"/Users/felixgerschner/git/ai-prototype/data/source/caltech101\" # Works\n",
    "# path = \"/Users/felixgerschner/git/ai-prototype/data/source/dagm\" # Works\n",
    "# path = \"/Users/felixgerschner/git/ai-prototype/data/target/mechanicalseals_fulllight\" # Works\n",
    "\n",
    "# X, y = load_local_dataset(path, \".JPEG\", input_shape=None) # CRASHES KERNEL\n",
    "# X, y = load_local_dataset(path, \".jpg\", input_shape=(350, 350))\n",
    "# X, y = load_local_dataset(path, \".PNG\", input_shape=(350, 350))\n",
    "# X, y = load_local_dataset(path, \".png\", input_shape=(350, 350))\n",
    "# X_train, y_train, X_test, y_test = load_local_dataset_train_test(path, \".png\", input_shape=(350, 350))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape\n",
    "# X_train.shape, \" \", X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(X[2523].astype(\"uint8\"))\n",
    "# plt.show()\n",
    "# y[2523]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a1586fa20ae7adda0123f353fb2992b7c692c0cb0bab9f479772660986f9846"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
